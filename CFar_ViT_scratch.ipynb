{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXtG5zAfnfPWvbXy4gxqWI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justworkname/Tamil_lab_Chance_Gammill/blob/main/CFar_ViT_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vit CFar"
      ],
      "metadata": {
        "id": "2FXqCrADCQqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "kJDbBqU4CYB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchinfo"
      ],
      "metadata": {
        "id": "SU7I7A8aCP6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJmY9sjBAnFU",
        "outputId": "72576d60-ccb3-4335-f112-0d611647118e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.0.1+cu118\n",
            "torchvision version: 0.15.2+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets.folder import has_file_allowed_extension\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")\n",
        "from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "ogo31qxDCwen",
        "outputId": "5ae70882-f5ea-4431-faf5-5c66c9e64c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper parameters"
      ],
      "metadata": {
        "id": "r7jJb2tNDSyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfer learning\n",
        "# 2 classes?\n",
        "# add grad cam\n",
        "# pytorch lightning\n",
        "num_epochs=50\n",
        "batch_size=5\n",
        "img_size=32\n",
        "in_channels=3 #RGB\n",
        "patch_size=4\n",
        "embedding_dim=384 ## 384\n",
        "head=12 # flash attention # heads per feature is 64 embeding dim/64,32,16\n",
        "dropout=0.1\n",
        "hidden_size=1536 # 4 times embeiding dimesions\n",
        "layers = 8\n",
        "num_classes=10 # will be over written in data phase"
      ],
      "metadata": {
        "id": "9y42NAX7DVfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "iDQTrtAnDGvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform"
      ],
      "metadata": {
        "id": "QM6qcmbDD3g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "WE2gdMq7D2LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load"
      ],
      "metadata": {
        "id": "H7ZlgpUwDhPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "num_classes=len(class_names)\n",
        "train_loader, test_loader,class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1-aVu8fBQQ8",
        "outputId": "ba827336-408a-42b6-8ee0-a038e909c18f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7c9028bf73d0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7c9028bf7520>,\n",
              " ['airplane',\n",
              "  'automobile',\n",
              "  'bird',\n",
              "  'cat',\n",
              "  'deer',\n",
              "  'dog',\n",
              "  'frog',\n",
              "  'horse',\n",
              "  'ship',\n",
              "  'truck'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Image"
      ],
      "metadata": {
        "id": "l3ZRUn9tFHlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch, label_batch = next(iter(train_loader))\n",
        "\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "image.shape, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12gHJ6lTDjFy",
        "outputId": "b6df8ac4-9cfb-4bbc-91f0-24d2a570c785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 32, 32]), tensor(0))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "vUua2YzOFDyN",
        "outputId": "611b1034-7280-45a2-a9ee-f82e7fb9aef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patch embeding layer"
      ],
      "metadata": {
        "id": "3KOwBjFOGpix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=in_channels,\n",
        "                 patch_size=patch_size,\n",
        "                 embedding_dim=embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,  # Corrected typo from kernels_size to kernel_size\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        image_resolution = x.shape[-1]\n",
        "\n",
        "        x_patched =self.patcher(x)\n",
        "        x_flattened= self.flatten(x_patched)\n",
        "\n",
        "        return x_flattened.permute(0,2,1)"
      ],
      "metadata": {
        "id": "X4DKUPQQHNxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Layer"
      ],
      "metadata": {
        "id": "N5vbOrJiNDwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                       nhead=head,\n",
        "                                                       dim_feedforward=hidden_size,\n",
        "                                                       dropout=dropout,\n",
        "                                                       activation=\"gelu\",\n",
        "                                                       batch_first=True,\n",
        "                                                       norm_first=True)\n",
        "transformer_encoder_layer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyTnDq1VLaD4",
        "outputId": "206087e1-9857-4814-dc29-28fb3bf19126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerEncoderLayer(\n",
              "  (self_attn): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "  )\n",
              "  (linear1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (linear2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "  (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (dropout2): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Model"
      ],
      "metadata": {
        "id": "iBeZdG7iPoM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=transformer_encoder_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xELgmGkONfs1",
        "outputId": "91310f06-04ff-4d9f-8996-4e76d5ad4faf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===========================================================================\n",
              "Layer (type:depth-idx)                             Param #\n",
              "===========================================================================\n",
              "TransformerEncoderLayer                            --\n",
              "├─MultiheadAttention: 1-1                          443,520\n",
              "│    └─NonDynamicallyQuantizableLinear: 2-1        147,840\n",
              "├─Linear: 1-2                                      591,360\n",
              "├─Dropout: 1-3                                     --\n",
              "├─Linear: 1-4                                      590,208\n",
              "├─LayerNorm: 1-5                                   768\n",
              "├─LayerNorm: 1-6                                   768\n",
              "├─Dropout: 1-7                                     --\n",
              "├─Dropout: 1-8                                     --\n",
              "===========================================================================\n",
              "Total params: 1,774,464\n",
              "Trainable params: 1,774,464\n",
              "Non-trainable params: 0\n",
              "==========================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stack Transformer Encoder"
      ],
      "metadata": {
        "id": "NiL_yNSdPsg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_encoder = nn.TransformerEncoder(\n",
        "    encoder_layer=transformer_encoder_layer,\n",
        "    num_layers=layers)\n"
      ],
      "metadata": {
        "id": "uAn5mtzFPVFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=transformer_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH_IXS-EQdEG",
        "outputId": "ceb7e6ea-1696-4f7c-f24b-25637f9fea7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================\n",
              "Layer (type:depth-idx)                                       Param #\n",
              "=====================================================================================\n",
              "TransformerEncoder                                           --\n",
              "├─ModuleList: 1-1                                            --\n",
              "│    └─TransformerEncoderLayer: 2-1                          --\n",
              "│    │    └─MultiheadAttention: 3-1                          591,360\n",
              "│    │    └─Linear: 3-2                                      591,360\n",
              "│    │    └─Dropout: 3-3                                     --\n",
              "│    │    └─Linear: 3-4                                      590,208\n",
              "│    │    └─LayerNorm: 3-5                                   768\n",
              "│    │    └─LayerNorm: 3-6                                   768\n",
              "│    │    └─Dropout: 3-7                                     --\n",
              "│    │    └─Dropout: 3-8                                     --\n",
              "│    └─TransformerEncoderLayer: 2-2                          --\n",
              "│    │    └─MultiheadAttention: 3-9                          591,360\n",
              "│    │    └─Linear: 3-10                                     591,360\n",
              "│    │    └─Dropout: 3-11                                    --\n",
              "│    │    └─Linear: 3-12                                     590,208\n",
              "│    │    └─LayerNorm: 3-13                                  768\n",
              "│    │    └─LayerNorm: 3-14                                  768\n",
              "│    │    └─Dropout: 3-15                                    --\n",
              "│    │    └─Dropout: 3-16                                    --\n",
              "│    └─TransformerEncoderLayer: 2-3                          --\n",
              "│    │    └─MultiheadAttention: 3-17                         591,360\n",
              "│    │    └─Linear: 3-18                                     591,360\n",
              "│    │    └─Dropout: 3-19                                    --\n",
              "│    │    └─Linear: 3-20                                     590,208\n",
              "│    │    └─LayerNorm: 3-21                                  768\n",
              "│    │    └─LayerNorm: 3-22                                  768\n",
              "│    │    └─Dropout: 3-23                                    --\n",
              "│    │    └─Dropout: 3-24                                    --\n",
              "│    └─TransformerEncoderLayer: 2-4                          --\n",
              "│    │    └─MultiheadAttention: 3-25                         591,360\n",
              "│    │    └─Linear: 3-26                                     591,360\n",
              "│    │    └─Dropout: 3-27                                    --\n",
              "│    │    └─Linear: 3-28                                     590,208\n",
              "│    │    └─LayerNorm: 3-29                                  768\n",
              "│    │    └─LayerNorm: 3-30                                  768\n",
              "│    │    └─Dropout: 3-31                                    --\n",
              "│    │    └─Dropout: 3-32                                    --\n",
              "│    └─TransformerEncoderLayer: 2-5                          --\n",
              "│    │    └─MultiheadAttention: 3-33                         591,360\n",
              "│    │    └─Linear: 3-34                                     591,360\n",
              "│    │    └─Dropout: 3-35                                    --\n",
              "│    │    └─Linear: 3-36                                     590,208\n",
              "│    │    └─LayerNorm: 3-37                                  768\n",
              "│    │    └─LayerNorm: 3-38                                  768\n",
              "│    │    └─Dropout: 3-39                                    --\n",
              "│    │    └─Dropout: 3-40                                    --\n",
              "│    └─TransformerEncoderLayer: 2-6                          --\n",
              "│    │    └─MultiheadAttention: 3-41                         591,360\n",
              "│    │    └─Linear: 3-42                                     591,360\n",
              "│    │    └─Dropout: 3-43                                    --\n",
              "│    │    └─Linear: 3-44                                     590,208\n",
              "│    │    └─LayerNorm: 3-45                                  768\n",
              "│    │    └─LayerNorm: 3-46                                  768\n",
              "│    │    └─Dropout: 3-47                                    --\n",
              "│    │    └─Dropout: 3-48                                    --\n",
              "│    └─TransformerEncoderLayer: 2-7                          --\n",
              "│    │    └─MultiheadAttention: 3-49                         591,360\n",
              "│    │    └─Linear: 3-50                                     591,360\n",
              "│    │    └─Dropout: 3-51                                    --\n",
              "│    │    └─Linear: 3-52                                     590,208\n",
              "│    │    └─LayerNorm: 3-53                                  768\n",
              "│    │    └─LayerNorm: 3-54                                  768\n",
              "│    │    └─Dropout: 3-55                                    --\n",
              "│    │    └─Dropout: 3-56                                    --\n",
              "│    └─TransformerEncoderLayer: 2-8                          --\n",
              "│    │    └─MultiheadAttention: 3-57                         591,360\n",
              "│    │    └─Linear: 3-58                                     591,360\n",
              "│    │    └─Dropout: 3-59                                    --\n",
              "│    │    └─Linear: 3-60                                     590,208\n",
              "│    │    └─LayerNorm: 3-61                                  768\n",
              "│    │    └─LayerNorm: 3-62                                  768\n",
              "│    │    └─Dropout: 3-63                                    --\n",
              "│    │    └─Dropout: 3-64                                    --\n",
              "=====================================================================================\n",
              "Total params: 14,195,712\n",
              "Trainable params: 14,195,712\n",
              "Non-trainable params: 0\n",
              "====================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT"
      ],
      "metadata": {
        "id": "c2Ypw4K4Q-1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=img_size,\n",
        "                 in_channels=in_channels,\n",
        "                 patch_size=patch_size,\n",
        "                 embedding_dim=embedding_dim,\n",
        "                 dropout=dropout,\n",
        "                 hidden_size=hidden_size,\n",
        "                 layers=layers,\n",
        "                 head=head,\n",
        "                 num_classes=num_classes):\n",
        "      super().__init__()\n",
        "\n",
        "      # patch embedding\n",
        "      self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                           patch_size=patch_size,\n",
        "                                           embedding_dim=embedding_dim)\n",
        "      # class token\n",
        "      self.class_token = nn.Parameter(torch.randn(1,1,embedding_dim),\n",
        "                                      requires_grad=True)\n",
        "      # positional Embedding\n",
        "      num_patches = img_size**2 // patch_size**2\n",
        "      self.positional_embedding = nn.Parameter(torch.randn(1, num_patches+1, embedding_dim))\n",
        "\n",
        "      # Create Patch + Positional embedding\n",
        "      self.embedding_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "      # Create stack TransformerEncoder Layers\n",
        "      self.transformer_encoder = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                                                                nhead=head,\n",
        "                                                                                                dim_feedforward=hidden_size,\n",
        "                                                                                                activation=\"gelu\",\n",
        "                                                                                                batch_first=True,\n",
        "                                                                                                norm_first=True),\n",
        "                                                        num_layers=layers)\n",
        "      # MLP head\n",
        "      self.mlp_head = nn.Sequential(\n",
        "          nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "          nn.Linear(in_features=embedding_dim,\n",
        "                    out_features=num_classes)\n",
        "      )\n",
        "\n",
        "    def forward(self,x):\n",
        "      # batch size\n",
        "      batch_size = x.shape[0]\n",
        "\n",
        "      # patch embedding\n",
        "      x = self.patch_embedding(x)\n",
        "\n",
        "      # expand class token\n",
        "      class_token = self.class_token.expand(batch_size,-1,-1)\n",
        "\n",
        "      # Prepend the class token to the patch embedding\n",
        "      x = torch.cat(( class_token,x), dim=1)\n",
        "\n",
        "      # add the positional embedding to patch embedding\n",
        "      x = self.positional_embedding + x\n",
        "\n",
        "      # dropout on patch\n",
        "      x = self.embedding_dropout(x)\n",
        "\n",
        "      # Pass embeding through transformer stack\n",
        "      x = self.transformer_encoder(x)\n",
        "\n",
        "      # pass 0th index of x through MLP head\n",
        "      x = self.mlp_head(x[:,0])\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "4IVHKasWQlW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vit\n",
        "vit = ViT(num_classes=num_classes).to(device)"
      ],
      "metadata": {
        "id": "ztbtJ78vWorD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=ViT(num_classes=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVNSgrQVYUKc",
        "outputId": "4d079fe0-4980-4896-a754-1c1f088d6282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                                            Param #\n",
              "==========================================================================================\n",
              "ViT                                                               25,344\n",
              "├─PatchEmbedding: 1-1                                             --\n",
              "│    └─Conv2d: 2-1                                                18,816\n",
              "│    └─Flatten: 2-2                                               --\n",
              "├─Dropout: 1-2                                                    --\n",
              "├─TransformerEncoder: 1-3                                         --\n",
              "│    └─ModuleList: 2-3                                            --\n",
              "│    │    └─TransformerEncoderLayer: 3-1                          1,774,464\n",
              "│    │    └─TransformerEncoderLayer: 3-2                          1,774,464\n",
              "│    │    └─TransformerEncoderLayer: 3-3                          1,774,464\n",
              "│    │    └─TransformerEncoderLayer: 3-4                          1,774,464\n",
              "│    │    └─TransformerEncoderLayer: 3-5                          1,774,464\n",
              "│    │    └─TransformerEncoderLayer: 3-6                          1,774,464\n",
              "│    │    └─TransformerEncoderLayer: 3-7                          1,774,464\n",
              "│    │    └─TransformerEncoderLayer: 3-8                          1,774,464\n",
              "├─Sequential: 1-4                                                 --\n",
              "│    └─LayerNorm: 2-4                                             768\n",
              "│    └─Linear: 2-5                                                1,155\n",
              "==========================================================================================\n",
              "Total params: 14,241,795\n",
              "Trainable params: 14,241,795\n",
              "Non-trainable params: 0\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "BKFCT7C-aMCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vit\n",
        "vit = ViT(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(vit.parameters(), lr=0.0005, weight_decay=0.05)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Initialize\n",
        "    batch_count= 0\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Train\n",
        "    vit.train()\n",
        "\n",
        "    # batch loop\n",
        "    bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    bar.set_description(f\"Epoch {epoch}\")\n",
        "    seen_examples = 0\n",
        "    for batch_idx, (data, target) in bar:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        batch_count=batch_count+1\n",
        "        optimizer.zero_grad()\n",
        "        output = vit(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Corect prediction count\n",
        "        _, predicted = output.max(1)\n",
        "\n",
        "        correct_predictions += predicted.eq(target).sum().item()\n",
        "        seen_examples += len(data)\n",
        "\n",
        "        bar.set_postfix(loss=loss.item(), accuracy=100. * correct_predictions / seen_examples)\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    training_accuracy = correct_predictions / len(train_loader.dataset)\n",
        "\n",
        "    # Initialize counters for loss and correct predictions in validation\n",
        "    val_loss = 0.0\n",
        "    val_correct_predictions = 0\n",
        "\n",
        "    # Set the model to evaluation mode (no gradient calculation)\n",
        "    vit.eval()\n",
        "\n",
        "    # Iterate over the validation data\n",
        "    with torch.no_grad():\n",
        "        for val_data, val_target in test_loader:\n",
        "            val_data=val_data.to(device)\n",
        "            val_target=val_target.to(device)\n",
        "            val_output = vit(val_data)\n",
        "            val_loss += criterion(val_output, val_target).item()\n",
        "\n",
        "            # Calculate the number of correct predictions in this validation batch\n",
        "            _, val_predicted = val_output.max(1)\n",
        "            val_correct_predictions += val_predicted.eq(val_target).sum().item()\n",
        "\n",
        "    # Calculate validation accuracy\n",
        "    validation_accuracy = val_correct_predictions / len(test_loader.dataset)\n",
        "\n",
        "    # Print training and validation progress for this epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training Loss: {total_loss/batch_count:.4f} Training Accuracy: {training_accuracy:.4f} Validation Loss: {val_loss/len(test_loader):.4f} Validation Accuracy: {validation_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAVkqV2NaVuQ",
        "outputId": "88a5ad29-b85c-4b2d-f501-780cf0b7684e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 10000/10000 [05:31<00:00, 30.12it/s, accuracy=24.4, loss=1.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Training Loss: 2.0371 Training Accuracy: 0.2443 Validation Loss: 1.8590 Validation Accuracy: 0.3212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 10000/10000 [05:15<00:00, 31.67it/s, accuracy=33.5, loss=1.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/50] Training Loss: 1.8268 Training Accuracy: 0.3351 Validation Loss: 1.7699 Validation Accuracy: 0.3510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 10000/10000 [05:15<00:00, 31.68it/s, accuracy=37.5, loss=1.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/50] Training Loss: 1.7218 Training Accuracy: 0.3752 Validation Loss: 1.6955 Validation Accuracy: 0.3940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 10000/10000 [05:15<00:00, 31.74it/s, accuracy=40.5, loss=1.83]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/50] Training Loss: 1.6433 Training Accuracy: 0.4047 Validation Loss: 1.5900 Validation Accuracy: 0.4244\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 10000/10000 [05:15<00:00, 31.69it/s, accuracy=43.4, loss=2.07]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/50] Training Loss: 1.5735 Training Accuracy: 0.4336 Validation Loss: 1.5070 Validation Accuracy: 0.4558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 10000/10000 [05:15<00:00, 31.68it/s, accuracy=46, loss=1.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/50] Training Loss: 1.4897 Training Accuracy: 0.4603 Validation Loss: 1.4892 Validation Accuracy: 0.4681\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 10000/10000 [05:15<00:00, 31.73it/s, accuracy=48.5, loss=1.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/50] Training Loss: 1.4261 Training Accuracy: 0.4851 Validation Loss: 1.3867 Validation Accuracy: 0.5085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 10000/10000 [05:14<00:00, 31.77it/s, accuracy=50.5, loss=1.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/50] Training Loss: 1.3735 Training Accuracy: 0.5046 Validation Loss: 1.3906 Validation Accuracy: 0.5057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 10000/10000 [05:15<00:00, 31.71it/s, accuracy=52.2, loss=1.26]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/50] Training Loss: 1.3303 Training Accuracy: 0.5216 Validation Loss: 1.2825 Validation Accuracy: 0.5358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 10000/10000 [05:14<00:00, 31.76it/s, accuracy=53.6, loss=1.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50] Training Loss: 1.2928 Training Accuracy: 0.5359 Validation Loss: 1.2617 Validation Accuracy: 0.5452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 10000/10000 [05:14<00:00, 31.82it/s, accuracy=54.7, loss=2.31]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/50] Training Loss: 1.2607 Training Accuracy: 0.5469 Validation Loss: 1.2413 Validation Accuracy: 0.5521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 10000/10000 [05:14<00:00, 31.81it/s, accuracy=55.8, loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/50] Training Loss: 1.2315 Training Accuracy: 0.5579 Validation Loss: 1.2832 Validation Accuracy: 0.5437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 10000/10000 [05:15<00:00, 31.73it/s, accuracy=56.5, loss=1.81]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/50] Training Loss: 1.2064 Training Accuracy: 0.5655 Validation Loss: 1.2302 Validation Accuracy: 0.5554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 10000/10000 [05:16<00:00, 31.60it/s, accuracy=57.7, loss=1.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/50] Training Loss: 1.1834 Training Accuracy: 0.5770 Validation Loss: 1.1932 Validation Accuracy: 0.5666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 10000/10000 [05:15<00:00, 31.73it/s, accuracy=58.2, loss=0.824]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/50] Training Loss: 1.1636 Training Accuracy: 0.5824 Validation Loss: 1.1890 Validation Accuracy: 0.5700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 10000/10000 [05:14<00:00, 31.77it/s, accuracy=58.6, loss=0.982]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/50] Training Loss: 1.1589 Training Accuracy: 0.5861 Validation Loss: 1.2199 Validation Accuracy: 0.5717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 10000/10000 [05:15<00:00, 31.73it/s, accuracy=59.1, loss=1.34]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/50] Training Loss: 1.1452 Training Accuracy: 0.5914 Validation Loss: 1.1844 Validation Accuracy: 0.5799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 10000/10000 [05:17<00:00, 31.51it/s, accuracy=59.7, loss=0.44]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/50] Training Loss: 1.1304 Training Accuracy: 0.5974 Validation Loss: 1.2694 Validation Accuracy: 0.5650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 10000/10000 [05:16<00:00, 31.56it/s, accuracy=60, loss=1.58]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/50] Training Loss: 1.1185 Training Accuracy: 0.6004 Validation Loss: 1.1593 Validation Accuracy: 0.5882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 10000/10000 [05:16<00:00, 31.61it/s, accuracy=60.3, loss=2.29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/50] Training Loss: 1.1098 Training Accuracy: 0.6027 Validation Loss: 1.1307 Validation Accuracy: 0.5964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20: 100%|██████████| 10000/10000 [05:16<00:00, 31.61it/s, accuracy=60.2, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/50] Training Loss: 1.1054 Training Accuracy: 0.6023 Validation Loss: 1.1402 Validation Accuracy: 0.5930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21: 100%|██████████| 10000/10000 [05:16<00:00, 31.63it/s, accuracy=61, loss=0.732]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/50] Training Loss: 1.0921 Training Accuracy: 0.6099 Validation Loss: 1.1183 Validation Accuracy: 0.5985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22: 100%|██████████| 10000/10000 [05:14<00:00, 31.76it/s, accuracy=61.3, loss=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/50] Training Loss: 1.0870 Training Accuracy: 0.6128 Validation Loss: 1.1468 Validation Accuracy: 0.6011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23: 100%|██████████| 10000/10000 [05:14<00:00, 31.78it/s, accuracy=61.4, loss=0.872]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/50] Training Loss: 1.0803 Training Accuracy: 0.6144 Validation Loss: 1.1054 Validation Accuracy: 0.6078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24: 100%|██████████| 10000/10000 [05:16<00:00, 31.55it/s, accuracy=61.3, loss=1.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/50] Training Loss: 1.0770 Training Accuracy: 0.6132 Validation Loss: 1.1704 Validation Accuracy: 0.5872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25: 100%|██████████| 10000/10000 [05:17<00:00, 31.52it/s, accuracy=62, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/50] Training Loss: 1.0645 Training Accuracy: 0.6201 Validation Loss: 1.0930 Validation Accuracy: 0.6061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26: 100%|██████████| 10000/10000 [05:16<00:00, 31.62it/s, accuracy=62.1, loss=1.85]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/50] Training Loss: 1.0624 Training Accuracy: 0.6209 Validation Loss: 1.1209 Validation Accuracy: 0.6032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27: 100%|██████████| 10000/10000 [05:16<00:00, 31.57it/s, accuracy=62.3, loss=0.851]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/50] Training Loss: 1.0543 Training Accuracy: 0.6230 Validation Loss: 1.1128 Validation Accuracy: 0.6052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28: 100%|██████████| 10000/10000 [05:16<00:00, 31.62it/s, accuracy=62.5, loss=1.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/50] Training Loss: 1.0490 Training Accuracy: 0.6250 Validation Loss: 1.0960 Validation Accuracy: 0.6015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29: 100%|██████████| 10000/10000 [05:18<00:00, 31.37it/s, accuracy=62.7, loss=1.76]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/50] Training Loss: 1.0408 Training Accuracy: 0.6272 Validation Loss: 1.1248 Validation Accuracy: 0.6066\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30: 100%|██████████| 10000/10000 [05:14<00:00, 31.77it/s, accuracy=63.2, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [31/50] Training Loss: 1.0376 Training Accuracy: 0.6322 Validation Loss: 1.0831 Validation Accuracy: 0.6188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31: 100%|██████████| 10000/10000 [05:18<00:00, 31.41it/s, accuracy=63.2, loss=1.02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [32/50] Training Loss: 1.0330 Training Accuracy: 0.6318 Validation Loss: 1.0784 Validation Accuracy: 0.6170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32: 100%|██████████| 10000/10000 [05:17<00:00, 31.49it/s, accuracy=63.4, loss=1.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [33/50] Training Loss: 1.0255 Training Accuracy: 0.6340 Validation Loss: 1.0749 Validation Accuracy: 0.6229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33: 100%|██████████| 10000/10000 [05:15<00:00, 31.65it/s, accuracy=63.6, loss=1.01]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [34/50] Training Loss: 1.0237 Training Accuracy: 0.6364 Validation Loss: 1.1040 Validation Accuracy: 0.6062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34: 100%|██████████| 10000/10000 [05:16<00:00, 31.61it/s, accuracy=63.7, loss=0.694]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [35/50] Training Loss: 1.0192 Training Accuracy: 0.6369 Validation Loss: 1.1146 Validation Accuracy: 0.6100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35: 100%|██████████| 10000/10000 [05:17<00:00, 31.54it/s, accuracy=63.8, loss=0.762]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [36/50] Training Loss: 1.0138 Training Accuracy: 0.6384 Validation Loss: 1.0633 Validation Accuracy: 0.6212\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36: 100%|██████████| 10000/10000 [05:16<00:00, 31.57it/s, accuracy=63.8, loss=1.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [37/50] Training Loss: 1.0123 Training Accuracy: 0.6380 Validation Loss: 1.0989 Validation Accuracy: 0.6037\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37: 100%|██████████| 10000/10000 [05:16<00:00, 31.63it/s, accuracy=64, loss=0.69]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [38/50] Training Loss: 1.0074 Training Accuracy: 0.6399 Validation Loss: 1.2086 Validation Accuracy: 0.5898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38: 100%|██████████| 10000/10000 [05:16<00:00, 31.56it/s, accuracy=64.3, loss=0.443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [39/50] Training Loss: 1.0059 Training Accuracy: 0.6426 Validation Loss: 1.0756 Validation Accuracy: 0.6151\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39: 100%|██████████| 10000/10000 [05:15<00:00, 31.71it/s, accuracy=64.3, loss=0.667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [40/50] Training Loss: 1.0025 Training Accuracy: 0.6431 Validation Loss: 1.0970 Validation Accuracy: 0.6206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40: 100%|██████████| 10000/10000 [05:15<00:00, 31.65it/s, accuracy=64.6, loss=1.28]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [41/50] Training Loss: 0.9943 Training Accuracy: 0.6459 Validation Loss: 1.1517 Validation Accuracy: 0.5891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41: 100%|██████████| 10000/10000 [05:15<00:00, 31.70it/s, accuracy=64.5, loss=1.56]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [42/50] Training Loss: 0.9994 Training Accuracy: 0.6451 Validation Loss: 1.1435 Validation Accuracy: 0.6012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42: 100%|██████████| 10000/10000 [05:16<00:00, 31.59it/s, accuracy=64.9, loss=1.03]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [43/50] Training Loss: 0.9875 Training Accuracy: 0.6494 Validation Loss: 1.0925 Validation Accuracy: 0.6136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43: 100%|██████████| 10000/10000 [05:14<00:00, 31.78it/s, accuracy=65, loss=0.759]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [44/50] Training Loss: 0.9864 Training Accuracy: 0.6496 Validation Loss: 1.1643 Validation Accuracy: 0.5913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44: 100%|██████████| 10000/10000 [05:17<00:00, 31.47it/s, accuracy=64.8, loss=0.583]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [45/50] Training Loss: 0.9866 Training Accuracy: 0.6476 Validation Loss: 1.0548 Validation Accuracy: 0.6234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45: 100%|██████████| 10000/10000 [05:16<00:00, 31.58it/s, accuracy=65, loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [46/50] Training Loss: 0.9829 Training Accuracy: 0.6500 Validation Loss: 1.1016 Validation Accuracy: 0.6096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46: 100%|██████████| 10000/10000 [05:16<00:00, 31.61it/s, accuracy=65.1, loss=0.981]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [47/50] Training Loss: 0.9818 Training Accuracy: 0.6512 Validation Loss: 1.0956 Validation Accuracy: 0.6077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47: 100%|██████████| 10000/10000 [05:15<00:00, 31.70it/s, accuracy=65.1, loss=1.49]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [48/50] Training Loss: 0.9783 Training Accuracy: 0.6513 Validation Loss: 1.1432 Validation Accuracy: 0.5867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48: 100%|██████████| 10000/10000 [05:17<00:00, 31.50it/s, accuracy=65.2, loss=1.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [49/50] Training Loss: 0.9755 Training Accuracy: 0.6519 Validation Loss: 1.1052 Validation Accuracy: 0.6110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49: 100%|██████████| 10000/10000 [05:15<00:00, 31.66it/s, accuracy=65.2, loss=0.906]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/50] Training Loss: 0.9726 Training Accuracy: 0.6520 Validation Loss: 1.1427 Validation Accuracy: 0.6114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-pJKFPXjXMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}