{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPendmCelmKi93b6s2R9QKA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justworkname/Tamil_lab_Chance_Gammill/blob/main/CFar_ViT_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vit CFar"
      ],
      "metadata": {
        "id": "2FXqCrADCQqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "kJDbBqU4CYB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchinfo"
      ],
      "metadata": {
        "id": "SU7I7A8aCP6O"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJmY9sjBAnFU",
        "outputId": "e54e7fe4-0b3d-4f89-d99f-16feb906db23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.0.1+cu118\n",
            "torchvision version: 0.15.2+cu118\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets.folder import has_file_allowed_extension\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")\n",
        "from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ogo31qxDCwen",
        "outputId": "06e22a96-92f3-413e-acae-f4013ef06916"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper parameters"
      ],
      "metadata": {
        "id": "r7jJb2tNDSyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=50\n",
        "batch_size=5\n",
        "img_size=32\n",
        "in_channels=3 #RGB\n",
        "patch_size=4\n",
        "embedding_dim=patch_size**2*3 ## just realized i did not account for teh RGB values on the other one and I am going to be annoyed with my self if that was the issue\n",
        "head=12\n",
        "dropout=0.1\n",
        "hidden_size=192\n",
        "layers = 8\n",
        "num_classes=10 # will be over written in data phase"
      ],
      "metadata": {
        "id": "9y42NAX7DVfL"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "iDQTrtAnDGvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform"
      ],
      "metadata": {
        "id": "QM6qcmbDD3g5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "WE2gdMq7D2LS"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load"
      ],
      "metadata": {
        "id": "H7ZlgpUwDhPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "num_classes=len(class_names)\n",
        "train_loader, test_loader,class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1-aVu8fBQQ8",
        "outputId": "f7f74810-e763-43b4-9f15-774137e43083"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7da1e0318f70>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7da1e031b100>,\n",
              " ['airplane',\n",
              "  'automobile',\n",
              "  'bird',\n",
              "  'cat',\n",
              "  'deer',\n",
              "  'dog',\n",
              "  'frog',\n",
              "  'horse',\n",
              "  'ship',\n",
              "  'truck'])"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize Image"
      ],
      "metadata": {
        "id": "l3ZRUn9tFHlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch, label_batch = next(iter(train_loader))\n",
        "\n",
        "image, label = image_batch[0], label_batch[0]\n",
        "\n",
        "image.shape, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12gHJ6lTDjFy",
        "outputId": "e2a772c5-77c7-40b0-a4c5-73308f403a0a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 32, 32]), tensor(6))"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "vUua2YzOFDyN",
        "outputId": "ca94a953-7ec7-473e-eba8-133132ae2dcc"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Patch embeding layer"
      ],
      "metadata": {
        "id": "3KOwBjFOGpix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=in_channels,\n",
        "                 patch_size=patch_size,\n",
        "                 embedding_dim=embedding_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,  # Corrected typo from kernels_size to kernel_size\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "    def forward(self,x):\n",
        "        image_resolution = x.shape[-1]\n",
        "\n",
        "        x_patched =self.patcher(x)\n",
        "        x_flattened= self.flatten(x_patched)\n",
        "\n",
        "        return x_flattened.permute(0,2,1)"
      ],
      "metadata": {
        "id": "X4DKUPQQHNxB"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer Encoder Layer"
      ],
      "metadata": {
        "id": "N5vbOrJiNDwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                       nhead=head,\n",
        "                                                       dim_feedforward=hidden_size,\n",
        "                                                       dropout=dropout,\n",
        "                                                       activation=\"gelu\",\n",
        "                                                       batch_first=True,\n",
        "                                                       norm_first=True)\n",
        "transformer_encoder_layer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyTnDq1VLaD4",
        "outputId": "a4f05a0d-cd47-41cd-af97-aba17b6d7e78"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerEncoderLayer(\n",
              "  (self_attn): MultiheadAttention(\n",
              "    (out_proj): NonDynamicallyQuantizableLinear(in_features=48, out_features=48, bias=True)\n",
              "  )\n",
              "  (linear1): Linear(in_features=48, out_features=192, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (linear2): Linear(in_features=192, out_features=48, bias=True)\n",
              "  (norm1): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
              "  (norm2): LayerNorm((48,), eps=1e-05, elementwise_affine=True)\n",
              "  (dropout1): Dropout(p=0.1, inplace=False)\n",
              "  (dropout2): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Model"
      ],
      "metadata": {
        "id": "iBeZdG7iPoM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=transformer_encoder_layer,\n",
        "        input_size=patch_embedding_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xELgmGkONfs1",
        "outputId": "0f2d2dd0-5a69-4e1c-fa8c-b2b887125cfc"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "TransformerEncoderLayer                  [5, 64, 48]               28,272\n",
              "==========================================================================================\n",
              "Total params: 28,272\n",
              "Trainable params: 28,272\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0\n",
              "==========================================================================================\n",
              "Input size (MB): 0.06\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.06\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stack Transformer Encoder"
      ],
      "metadata": {
        "id": "NiL_yNSdPsg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_encoder = nn.TransformerEncoder(\n",
        "    encoder_layer=transformer_encoder_layer,\n",
        "    num_layers=layers)\n"
      ],
      "metadata": {
        "id": "uAn5mtzFPVFX"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=transformer_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH_IXS-EQdEG",
        "outputId": "7ecfd54b-837d-46f6-9729-e85ddc0de199"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================\n",
              "Layer (type:depth-idx)                                       Param #\n",
              "=====================================================================================\n",
              "TransformerEncoder                                           --\n",
              "├─ModuleList: 1-1                                            --\n",
              "│    └─TransformerEncoderLayer: 2-1                          --\n",
              "│    │    └─MultiheadAttention: 3-1                          9,408\n",
              "│    │    └─Linear: 3-2                                      9,408\n",
              "│    │    └─Dropout: 3-3                                     --\n",
              "│    │    └─Linear: 3-4                                      9,264\n",
              "│    │    └─LayerNorm: 3-5                                   96\n",
              "│    │    └─LayerNorm: 3-6                                   96\n",
              "│    │    └─Dropout: 3-7                                     --\n",
              "│    │    └─Dropout: 3-8                                     --\n",
              "│    └─TransformerEncoderLayer: 2-2                          --\n",
              "│    │    └─MultiheadAttention: 3-9                          9,408\n",
              "│    │    └─Linear: 3-10                                     9,408\n",
              "│    │    └─Dropout: 3-11                                    --\n",
              "│    │    └─Linear: 3-12                                     9,264\n",
              "│    │    └─LayerNorm: 3-13                                  96\n",
              "│    │    └─LayerNorm: 3-14                                  96\n",
              "│    │    └─Dropout: 3-15                                    --\n",
              "│    │    └─Dropout: 3-16                                    --\n",
              "│    └─TransformerEncoderLayer: 2-3                          --\n",
              "│    │    └─MultiheadAttention: 3-17                         9,408\n",
              "│    │    └─Linear: 3-18                                     9,408\n",
              "│    │    └─Dropout: 3-19                                    --\n",
              "│    │    └─Linear: 3-20                                     9,264\n",
              "│    │    └─LayerNorm: 3-21                                  96\n",
              "│    │    └─LayerNorm: 3-22                                  96\n",
              "│    │    └─Dropout: 3-23                                    --\n",
              "│    │    └─Dropout: 3-24                                    --\n",
              "│    └─TransformerEncoderLayer: 2-4                          --\n",
              "│    │    └─MultiheadAttention: 3-25                         9,408\n",
              "│    │    └─Linear: 3-26                                     9,408\n",
              "│    │    └─Dropout: 3-27                                    --\n",
              "│    │    └─Linear: 3-28                                     9,264\n",
              "│    │    └─LayerNorm: 3-29                                  96\n",
              "│    │    └─LayerNorm: 3-30                                  96\n",
              "│    │    └─Dropout: 3-31                                    --\n",
              "│    │    └─Dropout: 3-32                                    --\n",
              "│    └─TransformerEncoderLayer: 2-5                          --\n",
              "│    │    └─MultiheadAttention: 3-33                         9,408\n",
              "│    │    └─Linear: 3-34                                     9,408\n",
              "│    │    └─Dropout: 3-35                                    --\n",
              "│    │    └─Linear: 3-36                                     9,264\n",
              "│    │    └─LayerNorm: 3-37                                  96\n",
              "│    │    └─LayerNorm: 3-38                                  96\n",
              "│    │    └─Dropout: 3-39                                    --\n",
              "│    │    └─Dropout: 3-40                                    --\n",
              "│    └─TransformerEncoderLayer: 2-6                          --\n",
              "│    │    └─MultiheadAttention: 3-41                         9,408\n",
              "│    │    └─Linear: 3-42                                     9,408\n",
              "│    │    └─Dropout: 3-43                                    --\n",
              "│    │    └─Linear: 3-44                                     9,264\n",
              "│    │    └─LayerNorm: 3-45                                  96\n",
              "│    │    └─LayerNorm: 3-46                                  96\n",
              "│    │    └─Dropout: 3-47                                    --\n",
              "│    │    └─Dropout: 3-48                                    --\n",
              "│    └─TransformerEncoderLayer: 2-7                          --\n",
              "│    │    └─MultiheadAttention: 3-49                         9,408\n",
              "│    │    └─Linear: 3-50                                     9,408\n",
              "│    │    └─Dropout: 3-51                                    --\n",
              "│    │    └─Linear: 3-52                                     9,264\n",
              "│    │    └─LayerNorm: 3-53                                  96\n",
              "│    │    └─LayerNorm: 3-54                                  96\n",
              "│    │    └─Dropout: 3-55                                    --\n",
              "│    │    └─Dropout: 3-56                                    --\n",
              "│    └─TransformerEncoderLayer: 2-8                          --\n",
              "│    │    └─MultiheadAttention: 3-57                         9,408\n",
              "│    │    └─Linear: 3-58                                     9,408\n",
              "│    │    └─Dropout: 3-59                                    --\n",
              "│    │    └─Linear: 3-60                                     9,264\n",
              "│    │    └─LayerNorm: 3-61                                  96\n",
              "│    │    └─LayerNorm: 3-62                                  96\n",
              "│    │    └─Dropout: 3-63                                    --\n",
              "│    │    └─Dropout: 3-64                                    --\n",
              "=====================================================================================\n",
              "Total params: 226,176\n",
              "Trainable params: 226,176\n",
              "Non-trainable params: 0\n",
              "====================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT"
      ],
      "metadata": {
        "id": "c2Ypw4K4Q-1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=img_size,\n",
        "                 in_channels=in_channels,\n",
        "                 patch_size=patch_size,\n",
        "                 embedding_dim=embedding_dim,\n",
        "                 dropout=dropout,\n",
        "                 hidden_size=hidden_size,\n",
        "                 layers=layers,\n",
        "                 head=head,\n",
        "                 num_classes=num_classes):\n",
        "      super().__init__()\n",
        "\n",
        "      # patch embedding\n",
        "      self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                           patch_size=patch_size,\n",
        "                                           embedding_dim=embedding_dim)\n",
        "      # class token\n",
        "      self.class_token = nn.Parameter(torch.randn(1,1,embedding_dim),\n",
        "                                      requires_grad=True)\n",
        "      # positional Embedding\n",
        "      num_patches = img_size**2 // patch_size**2\n",
        "      self.positional_embedding = nn.Parameter(torch.randn(1, num_patches+1, embedding_dim))\n",
        "\n",
        "      # Create Patch + Positional embedding\n",
        "      self.embedding_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "      # Create stack TransformerEncoder Layers\n",
        "      self.transformer_encoder = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                                                                nhead=head,\n",
        "                                                                                                dim_feedforward=hidden_size,\n",
        "                                                                                                activation=\"gelu\",\n",
        "                                                                                                batch_first=True,\n",
        "                                                                                                norm_first=True),\n",
        "                                                        num_layers=layers)\n",
        "      # MLP head\n",
        "      self.mlp_head = nn.Sequential(\n",
        "          nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "          nn.Linear(in_features=embedding_dim,\n",
        "                    out_features=num_classes)\n",
        "      )\n",
        "\n",
        "    def forward(self,x):\n",
        "      # batch size\n",
        "      batch_size = x.shape[0]\n",
        "\n",
        "      # patch embedding\n",
        "      x = self.patch_embedding(x)\n",
        "\n",
        "      # expand class token\n",
        "      class_token = self.class_token.expand(batch_size,-1,-1)\n",
        "\n",
        "      # Prepend the class token to the patch embedding\n",
        "      x = torch.cat(( class_token,x), dim=1)\n",
        "\n",
        "      # add the positional embedding to patch embedding\n",
        "      x = self.positional_embedding + x\n",
        "\n",
        "      # dropout on patch\n",
        "      x = self.embedding_dropout(x)\n",
        "\n",
        "      # Pass embeding through transformer stack\n",
        "      x = self.transformer_encoder(x)\n",
        "\n",
        "      # pass 0th index of x through MLP head\n",
        "      x = self.mlp_head(x[:,0])\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "4IVHKasWQlW8"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vit\n",
        "vit = ViT(num_classes=num_classes).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztbtJ78vWorD",
        "outputId": "0021133e-a2a8-427e-8845-c62c1bddf15e"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:405: UserWarning: Initializing zero-element tensors is a no-op\n",
            "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model=ViT(num_classes=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVNSgrQVYUKc",
        "outputId": "bffffd25-4ecc-44e0-97d9-456df8622944"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                                            Param #\n",
              "==========================================================================================\n",
              "ViT                                                               3,168\n",
              "├─PatchEmbedding: 1-1                                             --\n",
              "│    └─Conv2d: 2-1                                                2,352\n",
              "│    └─Flatten: 2-2                                               --\n",
              "├─Dropout: 1-2                                                    --\n",
              "├─TransformerEncoder: 1-3                                         --\n",
              "│    └─ModuleList: 2-3                                            --\n",
              "│    │    └─TransformerEncoderLayer: 3-1                          28,272\n",
              "│    │    └─TransformerEncoderLayer: 3-2                          28,272\n",
              "│    │    └─TransformerEncoderLayer: 3-3                          28,272\n",
              "│    │    └─TransformerEncoderLayer: 3-4                          28,272\n",
              "│    │    └─TransformerEncoderLayer: 3-5                          28,272\n",
              "│    │    └─TransformerEncoderLayer: 3-6                          28,272\n",
              "│    │    └─TransformerEncoderLayer: 3-7                          28,272\n",
              "│    │    └─TransformerEncoderLayer: 3-8                          28,272\n",
              "├─Sequential: 1-4                                                 --\n",
              "│    └─LayerNorm: 2-4                                             96\n",
              "│    └─Linear: 2-5                                                147\n",
              "==========================================================================================\n",
              "Total params: 231,939\n",
              "Trainable params: 231,939\n",
              "Non-trainable params: 0\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "BKFCT7C-aMCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Vit\n",
        "vit = ViT(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit.parameters(), lr=0.0005)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Initialize\n",
        "    batch_count= 0\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    # Train\n",
        "    vit.train()\n",
        "\n",
        "    # batch loop\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        batch_count=batch_count+1\n",
        "        optimizer.zero_grad()\n",
        "        output = vit(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Corect prediction count\n",
        "        _, predicted = output.max(1)\n",
        "        correct_predictions += predicted.eq(target).sum().item()\n",
        "\n",
        "        # Total Loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    training_accuracy = correct_predictions / len(train_loader.dataset)\n",
        "\n",
        "    # Print training progress including loss and accuracy for this epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average_Loss: {total_loss/batch_count:.4f} Training Accuracy: {training_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "YAVkqV2NaVuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtiYdkA0c9Dx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}